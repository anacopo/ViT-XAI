{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport cv2\nimport os\nimport random\nimport gc\nimport copy\nimport pickle\nimport sys\n\nfrom PIL import Image\nfrom time import time\nfrom dataclasses import dataclass, asdict\nfrom google.colab import drive\nfrom IPython.display import clear_output\nfrom collections import Counter\nfrom itertools import combinations\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\nfrom torchvision import models\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import VOCSegmentation\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\nimport torch\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, jaccard_score, f1_score, accuracy_score\n\nfrom skimage.io import imread\nfrom skimage.transform import resize","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T13:21:23.867243Z","iopub.execute_input":"2025-06-26T13:21:23.867678Z","iopub.status.idle":"2025-06-26T13:21:31.465127Z","shell.execute_reply.started":"2025-06-26T13:21:23.867643Z","shell.execute_reply":"2025-06-26T13:21:31.464480Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/hila-chefer/Transformer-Explainability.git\n\nos.chdir(f'./Transformer-Explainability')\n\n!pip install einops","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T13:21:38.323583Z","iopub.execute_input":"2025-06-26T13:21:38.324052Z","iopub.status.idle":"2025-06-26T13:21:43.561357Z","shell.execute_reply.started":"2025-06-26T13:21:38.324026Z","shell.execute_reply":"2025-06-26T13:21:43.560462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VOC Dataset","metadata":{}},{"cell_type":"code","source":"dataset = VOCSegmentation(root='data', year='2012', image_set='val', download=True, transform=transforms.ToTensor())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T13:21:48.235685Z","iopub.execute_input":"2025-06-26T13:21:48.236010Z","iopub.status.idle":"2025-06-26T13:23:23.354631Z","shell.execute_reply.started":"2025-06-26T13:21:48.235982Z","shell.execute_reply":"2025-06-26T13:23:23.353918Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"code","source":"def show_cam_on_image(img, mask):\n    # create heatmap from mask on image\n    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n    heatmap = np.float32(heatmap) / 255\n    cam = heatmap + np.float32(img)\n    cam = cam / np.max(cam)\n    return cam\n\ndef print_top_classes(predictions):\n    prob = torch.softmax(predictions, dim=1)\n    top_prob, top_class = prob.topk(1, dim=1)\n    return top_prob.item(), top_class.item()\n\ndef manipulate_img_and_mask(image, true_mask):\n    # Convert tensor image to numpy array and back to tensor after transformation\n    image_np = image.permute(1, 2, 0).numpy()\n    image_np = (image_np * 255).astype(np.uint8)\n\n    # Convert mask to numpy array and resize to match the image dimensions\n    true_mask_np = np.array(true_mask)\n    true_mask_np_resized = cv2.resize(true_mask_np, (image_np.shape[1], image_np.shape[0]), interpolation=cv2.INTER_NEAREST)\n\n    # Binarize the true mask (foreground is 1, background is 0)\n    true_mask_np_resized = (true_mask_np_resized > 0).astype(np.uint8)\n\n    # Convert images back to tensors\n    image_tensor = transforms.ToTensor()(image_np)\n\n    # Resize the original image to 224x224\n    transform_resize = transforms.Compose([\n        transforms.Resize((224, 224)),\n    ])\n    image_resized = transform_resize(image_tensor)\n\n    return image_resized, true_mask_np_resized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T13:24:06.369356Z","iopub.execute_input":"2025-06-26T13:24:06.369652Z","iopub.status.idle":"2025-06-26T13:24:06.377369Z","shell.execute_reply.started":"2025-06-26T13:24:06.369630Z","shell.execute_reply":"2025-06-26T13:24:06.376588Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"from baselines.ViT.ViT_explanation_generator import LRP\nfrom baselines.ViT.ViT_explanation_generator import Baselines\nfrom baselines.ViT.ViT_new import vit_base_patch16_224 as vit_LRP_new\nfrom baselines.ViT.ViT_LRP import vit_base_patch16_224 as vit_LRP\nfrom torchvision import transforms\n\nmodel_A = vit_LRP_new(pretrained=True).cuda()\nmodel_B = vit_LRP(pretrained=True).cuda()\n\nb = Baselines(model_A)\nattribution_generator = LRP(model_B)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T13:24:15.103266Z","iopub.execute_input":"2025-06-26T13:24:15.103576Z","iopub.status.idle":"2025-06-26T13:24:22.414881Z","shell.execute_reply.started":"2025-06-26T13:24:15.103553Z","shell.execute_reply":"2025-06-26T13:24:22.414181Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1Way","metadata":{}},{"cell_type":"code","source":"def generate_LRP(original_image, class_index=None):\n    transformer_attribution = attribution_generator.generate_LRP(original_image.unsqueeze(0).cuda(), method=\"transformer_attribution\", index=class_index).detach()\n    return transformer_attribution\n\ndef generate_saliency(original_image, class_index=None):\n    original_image.requires_grad_()\n    output = model_B(original_image.unsqueeze(0).cuda())\n    loss = output[0, class_index] if class_index is not None else output.max()\n    model_B.zero_grad()\n    loss.backward()\n    saliency = original_image.grad.data.abs().max(dim=0, keepdim=True)[0]\n    saliency = torch.nn.functional.interpolate(saliency.unsqueeze(0), size=(14, 14), mode='bilinear')\n    return saliency\n\ndef generate_rollout(input_image,class_index=None, start_layer=3):\n    transformer_attribution = b.generate_rollout(input_image.unsqueeze(0).cuda(), start_layer=start_layer)\n    return transformer_attribution\n\ndef generate_CAM(input_image, class_index=None):\n    transformer_attribution = b.generate_cam_attn(input_image.unsqueeze(0).cuda(), index=class_index)\n    return transformer_attribution\n\n# Utility function to combine attributions and visualize\ndef combine_and_visualize_attributions_1way(input_image, method, use_thresholding=True):\n    device = input_image.device\n    attr = method(input_image).reshape(1, 1, 14, 14).to(device)\n\n    combined_attr = torch.nn.functional.interpolate(attr, scale_factor=16, mode='bilinear')\n    combined_attr = combined_attr.reshape(224, 224).cpu().detach().numpy()\n    combined_attr = (combined_attr - combined_attr.min()) / (combined_attr.max() - combined_attr.min())\n\n    if use_thresholding:\n        combined_attr = combined_attr * 255\n        combined_attr = combined_attr.astype(np.uint8)\n        _, combined_attr = cv2.threshold(combined_attr, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n        combined_attr[combined_attr == 255] = 1\n\n    image_transformer_attribution = input_image.permute(1, 2, 0).cpu().detach().numpy()\n    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n    vis = show_cam_on_image(image_transformer_attribution, combined_attr)\n    vis = np.uint8(255 * vis)\n    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n    return vis, combined_attr\n\n# Function to visualize each method with different combine methods\ndef visualize_methods_1way(input_image, use_thresholding=True):\n    methods = {\n        'LRP': generate_LRP,\n        'saliency': generate_saliency,\n        'rollout': generate_rollout,\n        'CAM': generate_CAM,\n    }\n\n    output = model_A(input_image.unsqueeze(0).cuda())\n    class_index = output.argmax().item()\n\n    results = []\n    for method_name, method_func in methods.items():\n          vis, mask = combine_and_visualize_attributions_1way(input_image, lambda img: method_func(img), use_thresholding)\n          results.append((f\"{method_name}\", vis, mask))\n\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T13:24:49.319157Z","iopub.execute_input":"2025-06-26T13:24:49.319467Z","iopub.status.idle":"2025-06-26T13:24:49.330611Z","shell.execute_reply.started":"2025-06-26T13:24:49.319446Z","shell.execute_reply":"2025-06-26T13:24:49.329534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image, true_mask = dataset[20]\nimage_resized, true_mask_np_resized = manipulate_img_and_mask(image, true_mask)\n\n# Convert tensor image to numpy array\nimage_np_resized = image_resized.permute(1, 2, 0).numpy()\nimage_np_resized = (image_np_resized * 255).astype(np.uint8)\nresults = visualize_methods_1way(image_resized, use_thresholding=True)\n\nfig, axes = plt.subplots(len(results), 3, figsize=(15, len(results)*3))\nfor ax_row, (name, result, mask) in zip(axes, results):\n    ax_row[0].imshow(result)\n    ax_row[0].set_title(name)\n    ax_row[0].axis('off')\n\n    ax_row[1].imshow(mask, cmap='gray')\n    ax_row[1].set_title(f\"{name} - Mask\")\n    ax_row[1].axis('off')\n\n    ax_row[2].imshow((np.array(transforms.Resize((224, 224))(true_mask)) > 0).astype(np.uint8), cmap='gray')\n    ax_row[2].set_title(\"GT Lesion Mask\")\n    ax_row[2].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:37:08.795576Z","iopub.execute_input":"2025-05-07T08:37:08.795880Z","iopub.status.idle":"2025-05-07T08:37:11.895916Z","shell.execute_reply.started":"2025-05-07T08:37:08.795858Z","shell.execute_reply":"2025-05-07T08:37:11.894722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom torchvision import transforms\n\noutput_folder = \"1way_folder\"\nos.makedirs(output_folder, exist_ok=True)\n\nall_results_one_way = []\n\nfor idx, (image_data, lesion_data) in enumerate(dataset):\n    image_resized, true_mask_np_resized = manipulate_img_and_mask(image_data, lesion_data)\n    output = model_A(image_resized.unsqueeze(0).cuda())\n    top_prob, top_class = print_top_classes(output)\n\n    if top_prob > 0.85:\n        results = visualize_methods_1way(image_resized, use_thresholding=True)\n        if results != []:\n            for name, result, mask in results:\n                predicted_mask_np = (mask > 0.5).astype(np.uint8)\n                true_mask_resized = cv2.resize(true_mask_np_resized, (224, 224), interpolation=cv2.INTER_NEAREST)\n    \n                # Flatten the masks for metric calculation\n                true_mask_flat = true_mask_resized.flatten()\n                predicted_mask_flat = predicted_mask_np.flatten()\n    \n                jaccard = jaccard_score(true_mask_flat, predicted_mask_flat)\n                f1 = f1_score(true_mask_flat, predicted_mask_flat)\n                pixel_accuracy = accuracy_score(true_mask_flat, predicted_mask_flat)\n    \n                # Store metrics\n                all_results_one_way.append({\n                    \"Image Index\": idx,\n                    \"Method\": name,\n                    \"Jaccard Index (IoU)\": jaccard,\n                    \"F1 Score\": f1,\n                    \"Pixel Accuracy\": pixel_accuracy\n                })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T13:26:22.037549Z","iopub.execute_input":"2025-06-26T13:26:22.037887Z","iopub.status.idle":"2025-06-26T13:26:48.204483Z","shell.execute_reply.started":"2025-06-26T13:26:22.037863Z","shell.execute_reply":"2025-06-26T13:26:48.203542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_and_display_results(file_name_for_saving, results):\n  results_df = pd.DataFrame(results)\n  csv_path = file_name_for_saving\n  results_df.to_csv(csv_path, index=False)\n  print(f\"Results saved to {csv_path}\")\n\n  print(\"Statistics by Method and Combine Method:\")\n  stats = results_df.groupby(\"Method\")[[\"Jaccard Index (IoU)\", \"F1 Score\", \"Pixel Accuracy\"]].mean()\n  print(stats)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:51:11.608007Z","iopub.execute_input":"2025-05-07T12:51:11.608306Z","iopub.status.idle":"2025-05-07T12:51:11.613002Z","shell.execute_reply.started":"2025-05-07T12:51:11.608285Z","shell.execute_reply":"2025-05-07T12:51:11.612320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_and_display_results(\"VOCmetrics_results_1WAY.csv\", all_results_one_way)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T19:40:48.396968Z","iopub.execute_input":"2025-05-06T19:40:48.397307Z","iopub.status.idle":"2025-05-06T19:40:48.453397Z","shell.execute_reply.started":"2025-05-06T19:40:48.397280Z","shell.execute_reply":"2025-05-06T19:40:48.452648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport cv2\nfrom sklearn.metrics import auc \n\ndef deletion_metric(model, image, attribution_map, class_index=None, steps=100):\n    \"\"\"\n    Computes the Deletion Metric for a given attribution map.\n\n    Parameters:\n    - model: Trained model used for classification.\n    - image: Input image tensor (C, H, W).\n    - attribution_map: The heatmap from CAM or LRP, normalized [0, 1].\n    - class_index: Class index to track model confidence for (optional).\n    - steps: Number of steps for iterative deletion.\n\n    Returns:\n    - auc_score: Area under the confidence curve (lower = better attribution).\n    - confidence_drop: List of model confidences after each deletion step.\n    \"\"\"\n    model.eval()\n\n    # Flatten the attribution map and sort pixel indices by importance (descending order)\n    importance_order = np.argsort(-attribution_map.flatten())\n\n    # Create a copy of the image for deletion process\n    image_np = image.permute(1, 2, 0).detach().cpu().numpy()  # Convert to (H, W, C)\n    modified_image = image_np.copy()\n\n    # Initial model confidence before deletion\n    with torch.no_grad():\n        output = model(image.unsqueeze(0).cuda())\n        if class_index is None:\n            class_index = output.argmax().item()\n        initial_confidence = torch.softmax(output, dim=1)[0, class_index].item()\n\n    confidence_drop = [initial_confidence]\n\n    # Deletion process: remove pixels in steps\n    total_pixels = image_np.shape[0] * image_np.shape[1]\n    pixels_per_step = total_pixels // steps\n\n    for step in range(1, steps + 1):\n        # Mask out the most important pixels\n        pixels_to_mask = importance_order[(step - 1) * pixels_per_step: step * pixels_per_step]\n\n        # Set those pixels to zero (blackout)\n        for idx in pixels_to_mask:\n            h, w = divmod(idx, image_np.shape[1])  # Convert 1D index to 2D coordinates\n            modified_image[h, w, :] = 0  # Black out across all channels\n\n        # Convert modified image back to tensor\n        modified_image_tensor = torch.from_numpy(modified_image).permute(2, 0, 1).float().cuda()\n\n        # Recalculate model confidence\n        with torch.no_grad():\n            output = model(modified_image_tensor.unsqueeze(0))\n            confidence = torch.softmax(output, dim=1)[0, class_index].item()\n\n        confidence_drop.append(confidence)\n\n    # Calculate Area Under the Confidence Curve (AUC)\n    x_axis = np.linspace(0, 1, len(confidence_drop)) \n    auc_score = auc(x_axis, confidence_drop)\n\n    return auc_score, confidence_drop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:38:14.297800Z","iopub.execute_input":"2025-05-07T08:38:14.298264Z","iopub.status.idle":"2025-05-07T08:38:14.313244Z","shell.execute_reply.started":"2025-05-07T08:38:14.298230Z","shell.execute_reply":"2025-05-07T08:38:14.312234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_expl_results_one_way = []\nfor idx in tqdm(range(len(dataset)), desc=\"Processing\", unit=\"image\"):\n    image, true_mask = dataset[idx]\n\n    image_resized, true_mask_np_resized = manipulate_img_and_mask(image, true_mask)\n\n    # Convert tensor image to numpy array\n    image_np_resized = image_resized.permute(1, 2, 0).numpy()\n    image_np_resized = (image_np_resized * 255).astype(np.uint8)\n\n    # Get model prediction and probability\n    output = model_A(image_resized.unsqueeze(0).cuda())\n    top_prob, top_class = print_top_classes(output)\n    if top_prob > 0.85:\n        results = visualize_methods_1way(image_resized, use_thresholding=False)\n        if results != []:\n            for name, result, mask in results:\n\n                auccc, _ = deletion_metric(model_A, image_resized, mask)\n                all_expl_results_one_way.append({\n                \"Image Index\": idx,\n                \"Method\": name,\n                \"Deletion Accuracy\": auccc\n                })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T07:40:58.026583Z","iopub.execute_input":"2025-05-07T07:40:58.026911Z","iopub.status.idle":"2025-05-07T08:20:46.876888Z","shell.execute_reply.started":"2025-05-07T07:40:58.026885Z","shell.execute_reply":"2025-05-07T08:20:46.875923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_and_display_results1(file_name_for_saving, results):\n  results_df = pd.DataFrame(results)\n  csv_path = file_name_for_saving\n  results_df.to_csv(csv_path, index=False)\n  print(f\"Results saved to {csv_path}\")\n\n  print(\"Statistics by Method and Combine Method:\")\n  stats = results_df.groupby(\"Method\")[[\"Deletion Accuracy\"]].mean()\n  print(stats)\n\nsave_and_display_results1(\"VOCmetrics_expl_results_1WAY_NOThresholding.csv\", all_expl_results_one_way)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:23:42.949728Z","iopub.execute_input":"2025-05-07T08:23:42.950124Z","iopub.status.idle":"2025-05-07T08:23:43.003382Z","shell.execute_reply.started":"2025-05-07T08:23:42.950096Z","shell.execute_reply":"2025-05-07T08:23:43.002671Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2way","metadata":{}},{"cell_type":"code","source":"def combine_and_visualize_attributions_2way(input_image, method1, method2, combine_method='sqrt', use_thresholding=True):\n    device = input_image.device\n    attr1 = method1(input_image).reshape(1, 1, 14, 14).to(device)\n    attr2 = method2(input_image).reshape(1, 1, 14, 14).to(device)\n\n    if combine_method == 'sqrt':\n        combined_attr = torch.sqrt(attr1 * attr2)\n    elif combine_method == 'multiply':\n        combined_attr = attr1 * attr2\n\n    combined_attr = torch.nn.functional.interpolate(combined_attr, scale_factor=16, mode='bilinear')\n    combined_attr = combined_attr.reshape(224, 224).cpu().detach().numpy()\n    combined_attr = (combined_attr - combined_attr.min()) / (combined_attr.max() - combined_attr.min())\n\n    if use_thresholding:\n        combined_attr = combined_attr * 255\n        combined_attr = combined_attr.astype(np.uint8)\n        _, combined_attr = cv2.threshold(combined_attr, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n        combined_attr[combined_attr == 255] = 1\n\n    image_transformer_attribution = input_image.permute(1, 2, 0).cpu().detach().numpy()\n    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n    vis = show_cam_on_image(image_transformer_attribution, combined_attr)\n    vis = np.uint8(255 * vis)\n    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n    return vis, combined_attr\n\n# Example usage\ndef visualize_combined_methods_2way(input_image, method1_name, method2_name, class_index, combine_method='sqrt', use_thresholding=True):\n    methods = {\n        'LRP': generate_LRP,\n        'saliency': generate_saliency,\n        'rollout': generate_rollout,\n        'CAM': generate_CAM,\n    }\n    method1 = methods[method1_name]\n    method2 = methods[method2_name]\n\n    return combine_and_visualize_attributions_2way(input_image, lambda img: method1(img, class_index), lambda img: method2(img, class_index), combine_method, use_thresholding)\n\n# Function to visualize all 2-way combinations\ndef visualize_all_combinations_2way(input_image, combine_methods=['sqrt', 'multiply'], use_thresholding=True):\n    methods = ['LRP', 'saliency', 'rollout', 'CAM']\n    combinations_list = list(combinations(methods, 2))\n\n    # Determine the predicted class index\n    output = model_A(input_image.unsqueeze(0).cuda())\n    class_index = output.argmax().item()\n\n    results = []\n    for combo in combinations_list:\n        for combine_method in combine_methods:\n            vis, mask = visualize_combined_methods_2way(input_image, combo[0], combo[1], class_index, combine_method, use_thresholding)\n            results.append((f\"{' + '.join(combo)} ({combine_method})\", vis, mask))\n\n    return results\n\nimage, true_mask = dataset[20]\nimage_resized, true_mask_np_resized = manipulate_img_and_mask(image, true_mask)\n\n# Convert tensor image to numpy array\nimage_np_resized = image_resized.permute(1, 2, 0).numpy()\nimage_np_resized = (image_np_resized * 255).astype(np.uint8)\n\n# Visualize all combinations\nresults = visualize_all_combinations_2way(image_resized, combine_methods=['sqrt', 'multiply'], use_thresholding=True)\n\n# Display the results\nfig, axes = plt.subplots(len(results), 3, figsize=(15, len(results)*3))\nfor ax_row, (name, result, mask) in zip(axes, results):\n    ax_row[0].imshow(result)\n    ax_row[0].set_title(name)\n    ax_row[0].axis('off')\n\n    ax_row[1].imshow(mask, cmap='gray')\n    ax_row[1].set_title(f\"{name} - Mask\")\n    ax_row[1].axis('off')\n\n    ax_row[2].imshow((np.array(transforms.Resize((224, 224))(true_mask)) > 0).astype(np.uint8), cmap='gray')\n    ax_row[2].set_title(\"GT Lesion Mask\")\n    ax_row[2].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:37:41.422038Z","iopub.execute_input":"2025-05-07T08:37:41.422328Z","iopub.status.idle":"2025-05-07T08:37:47.170004Z","shell.execute_reply.started":"2025-05-07T08:37:41.422307Z","shell.execute_reply":"2025-05-07T08:37:47.168660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom torchvision import transforms\n\nall_results_two_way = []\n\nfor idx, (image_data, lesion_data) in enumerate(dataset):\n    image_resized, true_mask_np_resized = manipulate_img_and_mask(image_data, lesion_data)\n    output = model_A(image_resized.unsqueeze(0).cuda())\n    top_prob, top_class = print_top_classes(output)\n    \n    if top_prob > 0.85:\n        results = visualize_all_combinations_2way(image_resized, combine_methods=['sqrt', 'multiply'], use_thresholding=True)\n        if results != []:\n            for name, result, mask in results:\n                predicted_mask_np = (mask > 0.5).astype(np.uint8)\n                true_mask_resized = cv2.resize(true_mask_np_resized, (224, 224), interpolation=cv2.INTER_NEAREST)\n    \n                # Flatten the masks for metric calculation\n                true_mask_flat = true_mask_resized.flatten()\n                predicted_mask_flat = predicted_mask_np.flatten()\n    \n                jaccard = jaccard_score(true_mask_flat, predicted_mask_flat)\n                f1 = f1_score(true_mask_flat, predicted_mask_flat)\n                pixel_accuracy = accuracy_score(true_mask_flat, predicted_mask_flat)\n    \n                # Store metrics\n                all_results_two_way.append({\n                    \"Image Index\": idx,\n                    \"Method\": name,\n                    \"Jaccard Index (IoU)\": jaccard,\n                    \"F1 Score\": f1,\n                    \"Pixel Accuracy\": pixel_accuracy\n                })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T19:48:00.195285Z","iopub.execute_input":"2025-05-06T19:48:00.195588Z","iopub.status.idle":"2025-05-06T20:01:48.171195Z","shell.execute_reply.started":"2025-05-06T19:48:00.195564Z","shell.execute_reply":"2025-05-06T20:01:48.170229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_and_display_results(\"VOCmetrics_results_2WAY.csv\", all_results_two_way)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T20:02:23.519960Z","iopub.execute_input":"2025-05-06T20:02:23.520304Z","iopub.status.idle":"2025-05-06T20:02:23.557878Z","shell.execute_reply.started":"2025-05-06T20:02:23.520272Z","shell.execute_reply":"2025-05-06T20:02:23.557184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_expl_results_two_way = []\nfor idx in tqdm(range(len(dataset)), desc=\"Processing\", unit=\"image\"):\n    image, true_mask = dataset[idx]\n    image_resized, true_mask_np_resized = manipulate_img_and_mask(image, true_mask)\n\n    # Convert tensor image to numpy array\n    image_np_resized = image_resized.permute(1, 2, 0).numpy()\n    image_np_resized = (image_np_resized * 255).astype(np.uint8)\n\n    # Get model prediction and probability\n    output = model_A(image_resized.unsqueeze(0).cuda())\n    top_prob, top_class = print_top_classes(output)\n    if top_prob > 0.85:\n        results = visualize_all_combinations_2way(image_resized, combine_methods=['sqrt', 'multiply'], use_thresholding=False)\n        if results != []:\n            for name, result, mask in results:\n                auccc, _ = deletion_metric(model_A, image_resized, mask)\n                all_expl_results_two_way.append({\n                \"Image Index\": idx,\n                \"Method\": name,\n                \"Deletion Accuracy\": auccc\n                })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:38:47.856997Z","iopub.execute_input":"2025-05-07T08:38:47.857337Z","iopub.status.idle":"2025-05-07T10:54:16.334510Z","shell.execute_reply.started":"2025-05-07T08:38:47.857312Z","shell.execute_reply":"2025-05-07T10:54:16.333745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_and_display_results1(\"VOCmetrics_expl_results_2WAY_NOThresholding.csv\", all_expl_results_two_way)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:54:54.716799Z","iopub.execute_input":"2025-05-07T10:54:54.717102Z","iopub.status.idle":"2025-05-07T10:54:54.777758Z","shell.execute_reply.started":"2025-05-07T10:54:54.717079Z","shell.execute_reply":"2025-05-07T10:54:54.776994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3way","metadata":{}},{"cell_type":"code","source":"def combine_and_visualize_attributions_3way(input_image, methods, combine_method='sqrt', use_thresholding=True):\n    device = input_image.device\n    attributions = []\n    for method in methods:\n        if method.__name__ in ['generate_saliency', 'generate_CAM', 'generate_LRP']:\n            attr = method(input_image, class_index=1).reshape(1, 1, 14, 14).to(device)  # class_index is set to 1 for demonstration\n        else:\n            attr = method(input_image).reshape(1, 1, 14, 14).to(device)\n        attributions.append(attr)\n\n    if combine_method == 'sqrt':\n        combined_attr = torch.sqrt(attributions[0] * attributions[1] * attributions[2])\n    elif combine_method == 'multiply':\n        combined_attr = attributions[0] * attributions[1] * attributions[2]\n\n    combined_attr = torch.nn.functional.interpolate(combined_attr, scale_factor=16, mode='bilinear')\n    combined_attr = combined_attr.reshape(224, 224).cpu().detach().numpy()\n    combined_attr = (combined_attr - combined_attr.min()) / (combined_attr.max() - combined_attr.min())\n\n    if use_thresholding:\n        combined_attr = combined_attr * 255\n        combined_attr = combined_attr.astype(np.uint8)\n        _, combined_attr = cv2.threshold(combined_attr, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n        combined_attr[combined_attr == 255] = 1\n\n    image_transformer_attribution = input_image.permute(1, 2, 0).cpu().detach().numpy()\n    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n    vis = show_cam_on_image(image_transformer_attribution, combined_attr)\n    vis = np.uint8(255 * vis)\n    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n    return vis, combined_attr\n\n# Example usage\ndef visualize_combined_methods_3way(input_image, method_names, class_index, combine_method='sqrt', use_thresholding=True):\n    methods = {\n        'LRP': generate_LRP,\n        'saliency': generate_saliency,\n        'rollout': generate_rollout,\n        'CAM': generate_CAM,\n    }\n    selected_methods = [methods[name] for name in method_names]\n\n    return combine_and_visualize_attributions_3way(input_image, selected_methods, combine_method, use_thresholding)\n\n# Function to visualize all 3-way combinations\ndef visualize_all_combinations_3way(input_image, combine_methods=['sqrt', 'multiply'], use_thresholding=True):\n    methods = ['LRP', 'saliency', 'rollout', 'CAM']\n    combinations_list = list(combinations(methods, 3))\n\n    # Determine the predicted class index\n    output = model_A(input_image.unsqueeze(0).cuda())\n    class_index = output.argmax().item()\n    # print(f\"Predicted class index: {class_index}\")\n\n    results = []\n    for combo in combinations_list:\n        for combine_method in combine_methods:\n            # print(f\"Visualizing {' + '.join(combo)} with {combine_method}\")\n            vis, mask = visualize_combined_methods_3way(input_image, combo, class_index, combine_method, use_thresholding)\n            results.append((f\"{' + '.join(combo)} ({combine_method})\", vis, mask))\n\n    return results\n\nimage, true_mask = dataset[20]\nimage_resized, true_mask_np_resized = manipulate_img_and_mask(image, true_mask)\nimage_np_resized = image_resized.permute(1, 2, 0).numpy()\nimage_np_resized = (image_np_resized * 255).astype(np.uint8)\n\nresults = visualize_all_combinations_3way(image_resized, combine_methods=['sqrt', 'multiply'], use_thresholding=True)\n\n# Display the results\nfig, axes = plt.subplots(len(results), 3, figsize=(15, len(results)*3))\nfor ax_row, (name, result, mask) in zip(axes, results):\n    ax_row[0].imshow(result)\n    ax_row[0].set_title(name)\n    ax_row[0].axis('off')\n\n    ax_row[1].imshow(mask, cmap='gray')\n    ax_row[1].set_title(f\"{name} - Mask\")\n    ax_row[1].axis('off')\n\n    ax_row[2].imshow((np.array(transforms.Resize((224, 224))(true_mask)) > 0).astype(np.uint8), cmap='gray')\n    ax_row[2].set_title(\"GT Lesion Mask\")\n    ax_row[2].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:56:42.527620Z","iopub.execute_input":"2025-05-07T10:56:42.527945Z","iopub.status.idle":"2025-05-07T10:56:46.781847Z","shell.execute_reply.started":"2025-05-07T10:56:42.527921Z","shell.execute_reply":"2025-05-07T10:56:46.780758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_expl_results_three_way = []\nfor idx in tqdm(range(len(dataset)), desc=\"Processing\", unit=\"image\"):\n    image, true_mask = dataset[idx]\n    image_resized, true_mask_np_resized = manipulate_img_and_mask(image, true_mask)\n\n    # Convert tensor image to numpy array\n    image_np_resized = image_resized.permute(1, 2, 0).numpy()\n    image_np_resized = (image_np_resized * 255).astype(np.uint8)\n\n    # Get model prediction and probability\n    output = model_A(image_resized.unsqueeze(0).cuda())\n    top_prob, top_class = print_top_classes(output)\n    if top_prob > 0.85:\n        results = visualize_all_combinations_3way(image_resized, combine_methods=['sqrt', 'multiply'], use_thresholding=False)\n        if results != []:\n            for name, result, mask in results:\n                auccc, _ = deletion_metric(model_A, image_resized, mask)\n                all_expl_results_three_way.append({\n                \"Image Index\": idx,\n                \"Method\": name,\n                \"Deletion Accuracy\": auccc\n                })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:59:38.347180Z","iopub.execute_input":"2025-05-07T10:59:38.347525Z","iopub.status.idle":"2025-05-07T12:34:17.098720Z","shell.execute_reply.started":"2025-05-07T10:59:38.347495Z","shell.execute_reply":"2025-05-07T12:34:17.097920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_and_display_results1(\"VOCmetrics_expl_results_3WAY_NOThresholding.csv\", all_expl_results_three_way)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:35:28.800741Z","iopub.execute_input":"2025-05-07T12:35:28.801083Z","iopub.status.idle":"2025-05-07T12:35:28.823178Z","shell.execute_reply.started":"2025-05-07T12:35:28.801055Z","shell.execute_reply":"2025-05-07T12:35:28.822494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom torchvision import transforms\n\nall_results_three_way = []\n\nfor idx, (image_data, lesion_data) in tqdm(enumerate(dataset)):\n    image_resized, true_mask_np_resized = manipulate_img_and_mask(image_data, lesion_data)\n    output = model_A(image_resized.unsqueeze(0).cuda())\n    top_prob, top_class = print_top_classes(output)\n    \n    if top_prob > 0.85:\n        results = visualize_all_combinations_3way(image_resized, combine_methods=['sqrt', 'multiply'], use_thresholding=True)\n        if results != []:\n            for name, result, mask in results:\n                predicted_mask_np = (mask > 0.5).astype(np.uint8)\n                true_mask_resized = cv2.resize(true_mask_np_resized, (224, 224), interpolation=cv2.INTER_NEAREST)\n    \n                # Flatten the masks for metric calculation\n                true_mask_flat = true_mask_resized.flatten()\n                predicted_mask_flat = predicted_mask_np.flatten()\n    \n                jaccard = jaccard_score(true_mask_flat, predicted_mask_flat)\n                f1 = f1_score(true_mask_flat, predicted_mask_flat)\n                pixel_accuracy = accuracy_score(true_mask_flat, predicted_mask_flat)\n    \n                # Store metrics\n                all_results_three_way.append({\n                    \"Image Index\": idx,\n                    \"Method\": name,\n                    \"Jaccard Index (IoU)\": jaccard,\n                    \"F1 Score\": f1,\n                    \"Pixel Accuracy\": pixel_accuracy\n                })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:37:05.500810Z","iopub.execute_input":"2025-05-07T12:37:05.501132Z","iopub.status.idle":"2025-05-07T12:50:46.913567Z","shell.execute_reply.started":"2025-05-07T12:37:05.501105Z","shell.execute_reply":"2025-05-07T12:50:46.912681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_and_display_results(\"VOCmetrics_results_3WAY.csv\", all_results_three_way)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:51:27.177951Z","iopub.execute_input":"2025-05-07T12:51:27.178263Z","iopub.status.idle":"2025-05-07T12:51:27.209401Z","shell.execute_reply.started":"2025-05-07T12:51:27.178239Z","shell.execute_reply":"2025-05-07T12:51:27.208470Z"}},"outputs":[],"execution_count":null}]}