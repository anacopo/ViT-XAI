{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "jpeg_files_with_masks = []\n",
        "jpeg_files_with_student_masks = []\n",
        "extract_dir = \"/content/drive/MyDrive/research/Splits\"\n",
        "sam_mask_dir = \"/content/drive/MyDrive/research/sam_masks\"\n",
        "\n",
        "for slip_id in range(1, 11):\n",
        "    slip_path = os.path.join(extract_dir, f\"split_{slip_id}\", \"imagenet_data\")\n",
        "\n",
        "    nested_folders = glob.glob(os.path.join(slip_path, \"*\"))\n",
        "    for folder in nested_folders:\n",
        "        files = glob.glob(os.path.join(folder, \"*.JPEG\"))\n",
        "        for jpeg_path in files:\n",
        "            # Extract filename without extension\n",
        "            filename = os.path.basename(jpeg_path).replace(\".JPEG\", \"\")\n",
        "            # Construct corresponding mask path\n",
        "            mask_path = os.path.join(sam_mask_dir, f\"{filename}.png\")\n",
        "\n",
        "            # Check if the corresponding mask exists\n",
        "            if os.path.exists(mask_path):\n",
        "                jpeg_files_with_masks.append((jpeg_path, mask_path))\n",
        "\n",
        "print(f\"Paired {len(jpeg_files_with_masks)} images with SAM masks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJdJGSFtHxCu",
        "outputId": "dac628bb-9f86-46e8-aad7-752ea8904bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paired 176 images with SAM masks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "import copy\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "from PIL import Image\n",
        "from time import time\n",
        "from dataclasses import dataclass, asdict\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import VOCSegmentation\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, jaccard_score, f1_score, accuracy_score\n",
        "\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize"
      ],
      "metadata": {
        "id": "e2CHmeapKryi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hila-chefer/Transformer-Explainability.git\n",
        "\n",
        "os.chdir(f'./Transformer-Explainability')\n",
        "\n",
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuRDurU6KveK",
        "outputId": "1b8512f7-5db1-4ecb-e690-11da115c8027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Transformer-Explainability'...\n",
            "remote: Enumerating objects: 386, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 386 (delta 3), reused 2 (delta 2), pack-reused 381 (from 2)\u001b[K\n",
            "Receiving objects: 100% (386/386), 3.85 MiB | 33.69 MiB/s, done.\n",
            "Resolving deltas: 100% (194/194), done.\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_cam_on_image(img, mask):\n",
        "    # create heatmap from mask on image\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam = heatmap + np.float32(img)\n",
        "    cam = cam / np.max(cam)\n",
        "    return cam"
      ],
      "metadata": {
        "id": "ZEkKqIanKyL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from baselines.ViT.ViT_explanation_generator import LRP\n",
        "from baselines.ViT.ViT_explanation_generator import Baselines\n",
        "from baselines.ViT.ViT_new import vit_base_patch16_224 as vit_LRP_new\n",
        "from baselines.ViT.ViT_LRP import vit_base_patch16_224 as vit_LRP\n",
        "from torchvision import transforms\n",
        "\n",
        "model_A = vit_LRP_new(pretrained=True).cuda()\n",
        "model_B = vit_LRP(pretrained=True).cuda()\n",
        "\n",
        "b = Baselines(model_A)\n",
        "attribution_generator = LRP(model_B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpdoKfq_K_yc",
        "outputId": "eaaac499-18e9-4cb0-b543-a66b82f62843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth\" to /root/.cache/torch/hub/checkpoints/jx_vit_base_p16_224-80ecf9dd.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_LRP(original_image, class_index=None):\n",
        "    transformer_attribution = attribution_generator.generate_LRP(original_image.unsqueeze(0).cuda(), method=\"transformer_attribution\", index=class_index).detach()\n",
        "    return transformer_attribution\n",
        "\n",
        "def generate_saliency(original_image, class_index=None):\n",
        "    original_image.requires_grad_()\n",
        "    output = model_B(original_image.unsqueeze(0).cuda())\n",
        "    loss = output[0, class_index] if class_index is not None else output.max()\n",
        "    model_B.zero_grad()\n",
        "    loss.backward()\n",
        "    saliency = original_image.grad.data.abs().max(dim=0, keepdim=True)[0]\n",
        "    saliency = torch.nn.functional.interpolate(saliency.unsqueeze(0), size=(14, 14), mode='bilinear')\n",
        "    return saliency\n",
        "\n",
        "def generate_rollout(input_image,class_index=None, start_layer=3):\n",
        "    transformer_attribution = b.generate_rollout(input_image.unsqueeze(0).cuda(), start_layer=start_layer)\n",
        "    return transformer_attribution\n",
        "\n",
        "def generate_CAM(input_image, class_index=None):\n",
        "    transformer_attribution = b.generate_cam_attn(input_image.unsqueeze(0).cuda(), index=class_index)\n",
        "    return transformer_attribution\n",
        "\n",
        "# Utility function to combine attributions and visualize\n",
        "def combine_and_visualize_attributions_1way(input_image, method, use_thresholding=True):\n",
        "    device = input_image.device\n",
        "    attr = method(input_image).reshape(1, 1, 14, 14).to(device)\n",
        "\n",
        "    combined_attr = torch.nn.functional.interpolate(attr, scale_factor=16, mode='bilinear')\n",
        "    combined_attr = combined_attr.reshape(224, 224).cpu().detach().numpy()\n",
        "    combined_attr = (combined_attr - combined_attr.min()) / (combined_attr.max() - combined_attr.min())\n",
        "\n",
        "    if use_thresholding:\n",
        "        combined_attr = combined_attr * 255\n",
        "        combined_attr = combined_attr.astype(np.uint8)\n",
        "        _, combined_attr = cv2.threshold(combined_attr, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        combined_attr[combined_attr == 255] = 1\n",
        "\n",
        "    image_transformer_attribution = input_image.permute(1, 2, 0).cpu().detach().numpy()\n",
        "    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n",
        "    vis = show_cam_on_image(image_transformer_attribution, combined_attr)\n",
        "    vis = np.uint8(255 * vis)\n",
        "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
        "    return vis, combined_attr\n",
        "\n",
        "def compute_metrics(mask, gt):\n",
        "    inter = np.logical_and(gt, mask).sum()\n",
        "    union = np.logical_or(gt, mask).sum()\n",
        "    jaccard = inter / union if union else 0\n",
        "    tp = inter; fp = mask.sum() - tp; fn = gt.sum() - tp\n",
        "    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
        "    pix_acc = inter / gt.sum() if gt.sum() > 0 else 0\n",
        "    return jaccard, f1, pix_acc\n",
        "\n",
        "# Function to visualize each method with different combine methods\n",
        "def visualize_methods_1way(input_image, use_thresholding=True):\n",
        "    methods = {\n",
        "        'LRP': generate_LRP,\n",
        "        'saliency': generate_saliency,\n",
        "        'rollout': generate_rollout,\n",
        "        'CAM': generate_CAM,\n",
        "    }\n",
        "\n",
        "    # Determine the predicted class index\n",
        "    output = model_A(input_image.unsqueeze(0).cuda())\n",
        "    class_index = output.argmax().item()\n",
        "    # print(f\"Predicted class index: {class_index}\")\n",
        "\n",
        "    results = []\n",
        "    for method_name, method_func in methods.items():\n",
        "          # print(f\"Visualizing {method_name}\")\n",
        "          vis, mask = combine_and_visualize_attributions_1way(input_image, lambda img: method_func(img), use_thresholding)\n",
        "          results.append((f\"{method_name}\", vis, mask))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "RgXFZT4aLRLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_ = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "\n",
        "all_results_one_way = []\n",
        "for img_path, mask_paths in tqdm(jpeg_files_with_masks, desc=\"Processing images with masks\"):\n",
        "  img_bgr = cv2.imread(img_path)\n",
        "  img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "  tens_ = transform_(img_rgb)\n",
        "\n",
        "  true_mask = Image.open(mask_paths).convert(\"L\")\n",
        "  true_mask_resized = transforms.Resize((224, 224))(true_mask)\n",
        "  true_mask_np = (np.array(true_mask_resized) > 0).astype(np.uint8)\n",
        "\n",
        "  results = visualize_methods_1way(tens_, use_thresholding=True)\n",
        "  for name, result, mask in results:\n",
        "    iou, f1, px = compute_metrics(mask, true_mask_np)\n",
        "    all_results_one_way.append({\n",
        "            \"Image Path\": img_path,\n",
        "            \"Method\": name,\n",
        "            \"Jaccard Index (IoU)\": iou,\n",
        "            \"F1 Score\": f1,\n",
        "            \"Pixel Accuracy\": px\n",
        "        })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylqvEFThKhuO",
        "outputId": "fa1b109e-6f7e-4879-ed55-a2f09218c173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images with masks:   1%|          | 1/176 [00:00<01:11,  2.45it/s]<ipython-input-10-b2e335c31a1c>:34: RuntimeWarning: invalid value encountered in cast\n",
            "  combined_attr = combined_attr.astype(np.uint8)\n",
            "Processing images with masks: 100%|██████████| 176/176 [03:31<00:00,  1.20s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(all_results_one_way)\n",
        "results_df.to_csv(\"imagenet_sam_1way_total.csv\", index=False)\n",
        "stats = results_df.groupby(\"Method\")[[\"Jaccard Index (IoU)\", \"F1 Score\", \"Pixel Accuracy\"]].mean()\n",
        "stats = stats.reset_index()\n",
        "\n",
        "print(stats)\n",
        "stats.to_csv(\"imagenetsam_1way.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b79XPFTpd0UP",
        "outputId": "2d96889f-4477-4291-f497-afe1db24cf3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Method  Jaccard Index (IoU)  F1 Score  Pixel Accuracy\n",
            "0       CAM             0.143344  0.212258        0.193979\n",
            "1       LRP             0.425939  0.567156        0.533221\n",
            "2   rollout             0.365452  0.507887        0.663221\n",
            "3  saliency             0.079444  0.127842        0.133708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "def deletion_metric(model, image, attribution_map, class_index=None, steps=100):\n",
        "    \"\"\"\n",
        "    Computes the Deletion Metric for a given attribution map.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained model used for classification.\n",
        "    - image: Input image tensor (C, H, W).\n",
        "    - attribution_map: The heatmap from CAM or LRP, normalized [0, 1].\n",
        "    - class_index: Class index to track model confidence for (optional).\n",
        "    - steps: Number of steps for iterative deletion.\n",
        "\n",
        "    Returns:\n",
        "    - auc_score: Area under the confidence curve (lower = better attribution).\n",
        "    - confidence_drop: List of model confidences after each deletion step.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Flatten the attribution map and sort pixel indices by importance (descending order)\n",
        "    importance_order = np.argsort(-attribution_map.flatten())\n",
        "\n",
        "    # Create a copy of the image for deletion process\n",
        "    image_np = image.permute(1, 2, 0).detach().cpu().numpy()  # Convert to (H, W, C)\n",
        "    modified_image = image_np.copy()\n",
        "\n",
        "    # Initial model confidence before deletion\n",
        "    with torch.no_grad():\n",
        "        output = model(image.unsqueeze(0).cuda())\n",
        "        if class_index is None:\n",
        "            class_index = output.argmax().item()\n",
        "        initial_confidence = torch.softmax(output, dim=1)[0, class_index].item()\n",
        "\n",
        "    confidence_drop = [initial_confidence]\n",
        "    # print(f\"Initial Confidence {initial_confidence}\")\n",
        "\n",
        "    # Deletion process: remove pixels in steps\n",
        "    total_pixels = image_np.shape[0] * image_np.shape[1]\n",
        "    pixels_per_step = total_pixels // steps\n",
        "\n",
        "    for step in range(1, steps + 1):\n",
        "        # Mask out the most important pixels\n",
        "        pixels_to_mask = importance_order[(step - 1) * pixels_per_step: step * pixels_per_step]\n",
        "\n",
        "        # Set those pixels to zero (blackout)\n",
        "        for idx in pixels_to_mask:\n",
        "            h, w = divmod(idx, image_np.shape[1])  # Convert 1D index to 2D coordinates\n",
        "            modified_image[h, w, :] = 0  # Black out across all channels\n",
        "\n",
        "        # Convert modified image back to tensor\n",
        "        modified_image_tensor = torch.from_numpy(modified_image).permute(2, 0, 1).float().cuda()\n",
        "\n",
        "        # Recalculate model confidence\n",
        "        with torch.no_grad():\n",
        "            output = model(modified_image_tensor.unsqueeze(0))\n",
        "            confidence = torch.softmax(output, dim=1)[0, class_index].item()\n",
        "\n",
        "        confidence_drop.append(confidence)\n",
        "\n",
        "    # Calculate Area Under the Confidence Curve (AUC)\n",
        "    x_axis = np.linspace(0, 1, len(confidence_drop))  # Percentage of pixels deleted\n",
        "    auc_score = auc(x_axis, confidence_drop)\n",
        "    # print(f\"AUC {auc_score}\")\n",
        "\n",
        "    return auc_score, confidence_drop"
      ],
      "metadata": {
        "id": "NmSvgIBkJPby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_expl_results_one_way = []\n",
        "\n",
        "for img_path, mask_paths in tqdm(jpeg_files_with_masks, desc=\"Processing images with masks\"):\n",
        "  img_bgr = cv2.imread(img_path)\n",
        "  img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "  tens_ = transform_(img_rgb)\n",
        "\n",
        "  true_mask = Image.open(mask_paths).convert(\"L\")\n",
        "  true_mask_resized = transforms.Resize((224, 224))(true_mask)\n",
        "  true_mask_np = (np.array(true_mask_resized) > 0).astype(np.uint8)\n",
        "\n",
        "  results = visualize_methods_1way(tens_, use_thresholding=False)\n",
        "  for name, result, mask in results:\n",
        "    auccc, _ = deletion_metric(model_A, tens_, mask)\n",
        "    all_expl_results_one_way.append({\n",
        "    \"Image Index\": img_path,\n",
        "    \"Method\": name,\n",
        "    \"Deletion Accuracy\": auccc\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPVQ0ewgJWIM",
        "outputId": "00271ab6-57b5-4021-adf0-95012729b98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images with masks:   1%|          | 1/176 [00:09<27:08,  9.31s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:   6%|▌         | 10/176 [01:17<22:03,  7.97s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:   9%|▉         | 16/176 [02:01<19:52,  7.45s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  11%|█         | 19/176 [02:23<19:24,  7.42s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  11%|█▏        | 20/176 [02:31<19:17,  7.42s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  28%|██▊       | 49/176 [06:08<15:39,  7.40s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  38%|███▊      | 67/176 [08:23<13:21,  7.35s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  44%|████▍     | 78/176 [09:46<12:07,  7.42s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  69%|██████▉   | 121/176 [15:04<06:43,  7.33s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  70%|██████▉   | 123/176 [15:19<06:32,  7.40s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  85%|████████▍ | 149/176 [18:33<03:19,  7.40s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks: 100%|██████████| 176/176 [21:53<00:00,  7.46s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_and_display_results1(file_name_for_saving, results):\n",
        "  results_df = pd.DataFrame(results)\n",
        "  csv_path = file_name_for_saving\n",
        "  results_df.to_csv(csv_path, index=False)\n",
        "  print(f\"Results saved to {csv_path}\")\n",
        "\n",
        "  print(\"Statistics by Method and Combine Method:\")\n",
        "  stats = results_df.groupby(\"Method\")[[\"Deletion Accuracy\"]].mean()\n",
        "  print(stats)\n",
        "\n",
        "save_and_display_results1(\"NETmetrics_expl_results_1WAY_NOThresholding.csv\", all_expl_results_one_way)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLTXkzzrOPZA",
        "outputId": "2817c534-2937-411d-80f8-345abebd9055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to NETmetrics_expl_results_1WAY_NOThresholding.csv\n",
            "Statistics by Method and Combine Method:\n",
            "          Deletion Accuracy\n",
            "Method                     \n",
            "CAM                0.401487\n",
            "LRP                0.190977\n",
            "rollout            0.238535\n",
            "saliency           0.443203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2way"
      ],
      "metadata": {
        "id": "dq0H5pO8fok8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_and_visualize_attributions_2way(input_image, method1, method2, combine_method='sqrt', use_thresholding=True):\n",
        "    device = input_image.device\n",
        "    attr1 = method1(input_image).reshape(1, 1, 14, 14).to(device)\n",
        "    attr2 = method2(input_image).reshape(1, 1, 14, 14).to(device)\n",
        "\n",
        "    if combine_method == 'sqrt':\n",
        "        combined_attr = torch.sqrt(attr1 * attr2)\n",
        "    elif combine_method == 'multiply':\n",
        "        combined_attr = attr1 * attr2\n",
        "\n",
        "    combined_attr = torch.nn.functional.interpolate(combined_attr, scale_factor=16, mode='bilinear')\n",
        "    combined_attr = combined_attr.reshape(224, 224).cpu().detach().numpy()\n",
        "    combined_attr = (combined_attr - combined_attr.min()) / (combined_attr.max() - combined_attr.min())\n",
        "\n",
        "    if use_thresholding:\n",
        "        combined_attr = combined_attr * 255\n",
        "        combined_attr = combined_attr.astype(np.uint8)\n",
        "        _, combined_attr = cv2.threshold(combined_attr, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        combined_attr[combined_attr == 255] = 1\n",
        "\n",
        "    image_transformer_attribution = input_image.permute(1, 2, 0).cpu().detach().numpy()\n",
        "    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n",
        "    vis = show_cam_on_image(image_transformer_attribution, combined_attr)\n",
        "    vis = np.uint8(255 * vis)\n",
        "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
        "    return vis, combined_attr\n",
        "\n",
        "def visualize_combined_methods_2way(input_image, method1_name, method2_name, class_index, combine_method='sqrt', use_thresholding=True):\n",
        "    methods = {\n",
        "        'LRP': generate_LRP,\n",
        "        'saliency': generate_saliency,\n",
        "        'rollout': generate_rollout,\n",
        "        'CAM': generate_CAM,\n",
        "    }\n",
        "    method1 = methods[method1_name]\n",
        "    method2 = methods[method2_name]\n",
        "\n",
        "    return combine_and_visualize_attributions_2way(input_image, lambda img: method1(img, class_index), lambda img: method2(img, class_index), combine_method, use_thresholding)\n",
        "\n",
        "def visualize_all_combinations_2way(input_image, combine_methods=['sqrt', 'multiply'], use_thresholding=True):\n",
        "    methods = ['LRP', 'saliency', 'rollout', 'CAM']\n",
        "    combinations_list = list(combinations(methods, 2))\n",
        "\n",
        "    # Determine the predicted class index\n",
        "    output = model_A(input_image.unsqueeze(0).cuda())\n",
        "    class_index = output.argmax().item()\n",
        "\n",
        "    results = []\n",
        "    for combo in combinations_list:\n",
        "        for combine_method in combine_methods:\n",
        "            vis, mask = visualize_combined_methods_2way(input_image, combo[0], combo[1], class_index, combine_method, use_thresholding)\n",
        "            results.append((f\"{' + '.join(combo)} ({combine_method})\", vis, mask))\n",
        "\n",
        "    return results\n",
        "\n",
        "all_results_two_way = []\n",
        "for img_path, mask_paths in tqdm(jpeg_files_with_masks, desc=\"Processing images with masks\", mininterval=8.0):\n",
        "  img_bgr = cv2.imread(img_path)\n",
        "  img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "  tens_ = transform_(img_rgb)\n",
        "\n",
        "  true_mask = Image.open(mask_paths).convert(\"L\")\n",
        "  true_mask_resized = transforms.Resize((224, 224))(true_mask)\n",
        "  true_mask_np = (np.array(true_mask_resized) > 0).astype(np.uint8)\n",
        "\n",
        "  results = visualize_all_combinations_2way(tens_, combine_methods=['sqrt', 'multiply'], use_thresholding=True)\n",
        "  for name, result, mask in results:\n",
        "    iou, f1, px = compute_metrics(mask, true_mask_np)\n",
        "    all_results_two_way.append({\n",
        "            \"Image Path\": img_path,\n",
        "            \"Method\": name,\n",
        "            \"Jaccard Index (IoU)\": iou,\n",
        "            \"F1 Score\": f1,\n",
        "            \"Pixel Accuracy\": px\n",
        "        })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAoihB4yfqrU",
        "outputId": "0a00ff15-a10b-400b-e6db-4d984ee14e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images with masks:   0%|          | 0/176 [00:00<?, ?it/s]<ipython-input-12-6e1a8f1df898>:17: RuntimeWarning: invalid value encountered in cast\n",
            "  combined_attr = combined_attr.astype(np.uint8)\n",
            "Processing images with masks: 100%|██████████| 176/176 [06:12<00:00,  2.12s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(all_results_two_way)\n",
        "results_df.to_csv(\"imagenet_sam_2way_total.csv\", index=False)\n",
        "stats = results_df.groupby(\"Method\")[[\"Jaccard Index (IoU)\", \"F1 Score\", \"Pixel Accuracy\"]].mean()\n",
        "stats = stats.reset_index()\n",
        "print(stats)\n",
        "stats.to_csv(\"imagenetsam_2way.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peig7UhjgBRw",
        "outputId": "ba1ff1be-e9fe-43bb-c024-7548701f7110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           Method  Jaccard Index (IoU)  F1 Score  \\\n",
            "0            LRP + CAM (multiply)             0.130902  0.200635   \n",
            "1                LRP + CAM (sqrt)             0.216166  0.310348   \n",
            "2        LRP + rollout (multiply)             0.412267  0.547230   \n",
            "3            LRP + rollout (sqrt)             0.523251  0.657107   \n",
            "4       LRP + saliency (multiply)             0.217715  0.327822   \n",
            "5           LRP + saliency (sqrt)             0.369727  0.509925   \n",
            "6        rollout + CAM (multiply)             0.134932  0.201772   \n",
            "7            rollout + CAM (sqrt)             0.206309  0.290058   \n",
            "8       saliency + CAM (multiply)             0.091899  0.143442   \n",
            "9           saliency + CAM (sqrt)             0.163829  0.237325   \n",
            "10  saliency + rollout (multiply)             0.118137  0.187263   \n",
            "11      saliency + rollout (sqrt)             0.191790  0.293633   \n",
            "\n",
            "    Pixel Accuracy  \n",
            "0         0.159289  \n",
            "1         0.277168  \n",
            "2         0.489180  \n",
            "3         0.687073  \n",
            "4         0.279172  \n",
            "5         0.527372  \n",
            "6         0.171077  \n",
            "7         0.289527  \n",
            "8         0.118762  \n",
            "9         0.232517  \n",
            "10        0.193807  \n",
            "11        0.374740  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_expl_results_two_way = []\n",
        "\n",
        "for img_path, mask_paths in tqdm(jpeg_files_with_masks, desc=\"Processing images with masks\"):\n",
        "  img_bgr = cv2.imread(img_path)\n",
        "  img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "  tens_ = transform_(img_rgb)\n",
        "\n",
        "  true_mask = Image.open(mask_paths).convert(\"L\")\n",
        "  true_mask_resized = transforms.Resize((224, 224))(true_mask)\n",
        "  true_mask_np = (np.array(true_mask_resized) > 0).astype(np.uint8)\n",
        "\n",
        "  results = visualize_all_combinations_2way(tens_, combine_methods=['sqrt', 'multiply'], use_thresholding=False)\n",
        "  for name, result, mask in results:\n",
        "    auccc, _ = deletion_metric(model_A, tens_, mask)\n",
        "    all_expl_results_two_way.append({\n",
        "    \"Image Index\": img_path,\n",
        "    \"Method\": name,\n",
        "    \"Deletion Accuracy\": auccc\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3Nwh1QUUCmV",
        "outputId": "cb97b83c-7f69-475b-fb7b-3b05768b5987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images with masks:   1%|          | 1/176 [00:20<1:01:08, 20.96s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:   6%|▌         | 10/176 [03:25<56:31, 20.43s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:   9%|▉         | 16/176 [05:27<54:28, 20.43s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  11%|█         | 19/176 [06:28<53:17, 20.36s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  11%|█▏        | 20/176 [06:49<53:02, 20.40s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  28%|██▊       | 49/176 [16:41<43:14, 20.43s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  38%|███▊      | 67/176 [22:49<37:07, 20.44s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  44%|████▍     | 78/176 [26:34<33:29, 20.51s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  69%|██████▉   | 121/176 [41:12<18:48, 20.52s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  70%|██████▉   | 123/176 [41:53<18:02, 20.43s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks:  85%|████████▍ | 149/176 [50:44<09:11, 20.44s/it]<ipython-input-4-b26fb25961ad>:3: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
            "Processing images with masks: 100%|██████████| 176/176 [59:56<00:00, 20.43s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_and_display_results1(file_name_for_saving, results):\n",
        "  results_df = pd.DataFrame(results)\n",
        "  csv_path = file_name_for_saving\n",
        "  results_df.to_csv(csv_path, index=False)\n",
        "  print(f\"Results saved to {csv_path}\")\n",
        "  print(\"Statistics by Method and Combine Method:\")\n",
        "  stats = results_df.groupby(\"Method\")[[\"Deletion Accuracy\"]].mean()\n",
        "  print(stats)\n",
        "\n",
        "save_and_display_results1(\"NETmetrics_expl_results_2WAY_NOThresholding.csv\", all_expl_results_two_way)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05BGfKmcUQbb",
        "outputId": "48decca4-72ad-4af3-9ecc-8a2071d957ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to NETmetrics_expl_results_2WAY_NOThresholding.csv\n",
            "Statistics by Method and Combine Method:\n",
            "                               Deletion Accuracy\n",
            "Method                                          \n",
            "LRP + CAM (multiply)                    0.378963\n",
            "LRP + CAM (sqrt)                        0.373167\n",
            "LRP + rollout (multiply)                0.186527\n",
            "LRP + rollout (sqrt)                    0.180897\n",
            "LRP + saliency (multiply)               0.242719\n",
            "LRP + saliency (sqrt)                   0.230390\n",
            "rollout + CAM (multiply)                0.390902\n",
            "rollout + CAM (sqrt)                    0.385437\n",
            "saliency + CAM (multiply)               0.409378\n",
            "saliency + CAM (sqrt)                   0.401523\n",
            "saliency + rollout (multiply)           0.349698\n",
            "saliency + rollout (sqrt)               0.340655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3way"
      ],
      "metadata": {
        "id": "LSzabIJEikYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_and_visualize_attributions_3way(input_image, methods, combine_method='sqrt', use_thresholding=True):\n",
        "    device = input_image.device\n",
        "    attributions = []\n",
        "    for method in methods:\n",
        "        if method.__name__ in ['generate_saliency', 'generate_CAM', 'generate_LRP']:\n",
        "            attr = method(input_image, class_index=1).reshape(1, 1, 14, 14).to(device)  # class_index is set to 1 for demonstration\n",
        "        else:\n",
        "            attr = method(input_image).reshape(1, 1, 14, 14).to(device)\n",
        "        attributions.append(attr)\n",
        "\n",
        "    if combine_method == 'sqrt':\n",
        "        combined_attr = torch.sqrt(attributions[0] * attributions[1] * attributions[2])\n",
        "    elif combine_method == 'multiply':\n",
        "        combined_attr = attributions[0] * attributions[1] * attributions[2]\n",
        "\n",
        "    combined_attr = torch.nn.functional.interpolate(combined_attr, scale_factor=16, mode='bilinear')\n",
        "    combined_attr = combined_attr.reshape(224, 224).cpu().detach().numpy()\n",
        "    combined_attr = (combined_attr - combined_attr.min()) / (combined_attr.max() - combined_attr.min())\n",
        "\n",
        "    if use_thresholding:\n",
        "        combined_attr = combined_attr * 255\n",
        "        combined_attr = combined_attr.astype(np.uint8)\n",
        "        _, combined_attr = cv2.threshold(combined_attr, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        combined_attr[combined_attr == 255] = 1\n",
        "\n",
        "    image_transformer_attribution = input_image.permute(1, 2, 0).cpu().detach().numpy()\n",
        "    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n",
        "    vis = show_cam_on_image(image_transformer_attribution, combined_attr)\n",
        "    vis = np.uint8(255 * vis)\n",
        "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
        "    return vis, combined_attr\n",
        "\n",
        "# Example usage\n",
        "def visualize_combined_methods_3way(input_image, method_names, class_index, combine_method='sqrt', use_thresholding=True):\n",
        "    methods = {\n",
        "        'LRP': generate_LRP,\n",
        "        'saliency': generate_saliency,\n",
        "        'rollout': generate_rollout,\n",
        "        'CAM': generate_CAM,\n",
        "    }\n",
        "    selected_methods = [methods[name] for name in method_names]\n",
        "\n",
        "    return combine_and_visualize_attributions_3way(input_image, selected_methods, combine_method, use_thresholding)\n",
        "\n",
        "# Function to visualize all 3-way combinations\n",
        "def visualize_all_combinations_3way(input_image, combine_methods=['sqrt', 'multiply'], use_thresholding=True):\n",
        "    methods = ['LRP', 'saliency', 'rollout', 'CAM']\n",
        "    combinations_list = list(combinations(methods, 3))\n",
        "\n",
        "    # Determine the predicted class index\n",
        "    output = model_A(input_image.unsqueeze(0).cuda())\n",
        "    class_index = output.argmax().item()\n",
        "\n",
        "    results = []\n",
        "    for combo in combinations_list:\n",
        "        for combine_method in combine_methods:\n",
        "            vis, mask = visualize_combined_methods_3way(input_image, combo, class_index, combine_method, use_thresholding)\n",
        "            results.append((f\"{' + '.join(combo)} ({combine_method})\", vis, mask))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "all_results_three_way = []\n",
        "for img_path, mask_paths in tqdm(jpeg_files_with_masks, desc=\"Processing images with masks\", mininterval=8.0):\n",
        "  img_bgr = cv2.imread(img_path)\n",
        "  img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "  tens_ = transform_(img_rgb)\n",
        "\n",
        "  true_mask = Image.open(mask_paths).convert(\"L\")\n",
        "  true_mask_resized = transforms.Resize((224, 224))(true_mask)\n",
        "  true_mask_np = (np.array(true_mask_resized) > 0).astype(np.uint8)\n",
        "\n",
        "  results = visualize_all_combinations_3way(tens_, combine_methods=['sqrt', 'multiply'], use_thresholding=True)\n",
        "  for name, result, mask in results:\n",
        "    iou, f1, px = compute_metrics(mask, true_mask_np)\n",
        "    all_results_three_way.append({\n",
        "            \"Image Path\": img_path,\n",
        "            \"Method\": name,\n",
        "            \"Jaccard Index (IoU)\": iou,\n",
        "            \"F1 Score\": f1,\n",
        "            \"Pixel Accuracy\": px\n",
        "        })"
      ],
      "metadata": {
        "id": "iyd8HIgKimjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(all_results_three_way)\n",
        "results_df.to_csv(\"imagenet_sam_3way_total.csv\", index=False)\n",
        "stats = results_df.groupby(\"Method\")[[\"Jaccard Index (IoU)\", \"F1 Score\", \"Pixel Accuracy\"]].mean()\n",
        "stats = stats.reset_index()\n",
        "print(stats)\n",
        "stats.to_csv(\"imagenetsam_3way.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5_f9Z3DiwLa",
        "outputId": "0305b917-e816-4988-8d6b-a7582ac47a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                Method  Jaccard Index (IoU)  F1 Score  \\\n",
            "0       LRP + rollout + CAM (multiply)             0.218392  0.323411   \n",
            "1           LRP + rollout + CAM (sqrt)             0.375814  0.505797   \n",
            "2      LRP + saliency + CAM (multiply)             0.139217  0.216847   \n",
            "3          LRP + saliency + CAM (sqrt)             0.278574  0.398569   \n",
            "4  LRP + saliency + rollout (multiply)             0.201951  0.305029   \n",
            "5      LRP + saliency + rollout (sqrt)             0.374451  0.515473   \n",
            "6  saliency + rollout + CAM (multiply)             0.102894  0.159366   \n",
            "7      saliency + rollout + CAM (sqrt)             0.222965  0.318297   \n",
            "\n",
            "   Pixel Accuracy  \n",
            "0        0.257308  \n",
            "1        0.483598  \n",
            "2        0.170882  \n",
            "3        0.379368  \n",
            "4        0.252577  \n",
            "5        0.525543  \n",
            "6        0.138337  \n",
            "7        0.339636  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_expl_results_three_way = []\n",
        "\n",
        "for img_path, mask_paths in tqdm(jpeg_files_with_masks, desc=\"Processing images with masks\"):\n",
        "  img_bgr = cv2.imread(img_path)\n",
        "  img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "  tens_ = transform_(img_rgb)\n",
        "\n",
        "  true_mask = Image.open(mask_paths).convert(\"L\")\n",
        "  true_mask_resized = transforms.Resize((224, 224))(true_mask)\n",
        "  true_mask_np = (np.array(true_mask_resized) > 0).astype(np.uint8)\n",
        "\n",
        "  results = visualize_all_combinations_3way(tens_, combine_methods=['sqrt', 'multiply'], use_thresholding=False)\n",
        "  for name, result, mask in results:\n",
        "    auccc, _ = deletion_metric(model_A, tens_, mask)\n",
        "    all_expl_results_three_way.append({\n",
        "    \"Image Index\": img_path,\n",
        "    \"Method\": name,\n",
        "    \"Deletion Accuracy\": auccc\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA3srC36jafe",
        "outputId": "e86164f7-eadc-44b3-8677-229c2e3fc726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images with masks: 100%|██████████| 176/176 [41:59<00:00, 14.31s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_results1(\"NETmetrics_expl_results_3WAY_NOThresholding.csv\", all_expl_results_three_way)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqPcSp7AkgvP",
        "outputId": "e450ba5b-e53d-4d76-aedc-380895499907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to NETmetrics_expl_results_3WAY_NOThresholding.csv\n",
            "Statistics by Method and Combine Method:\n",
            "                                     Deletion Accuracy\n",
            "Method                                                \n",
            "LRP + rollout + CAM (multiply)                0.275882\n",
            "LRP + rollout + CAM (sqrt)                    0.263707\n",
            "LRP + saliency + CAM (multiply)               0.305666\n",
            "LRP + saliency + CAM (sqrt)                   0.291612\n",
            "LRP + saliency + rollout (multiply)           0.247940\n",
            "LRP + saliency + rollout (sqrt)               0.236018\n",
            "saliency + rollout + CAM (multiply)           0.337381\n",
            "saliency + rollout + CAM (sqrt)               0.320341\n"
          ]
        }
      ]
    }
  ]
}